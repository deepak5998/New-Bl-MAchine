{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intialized neural nework as sequential\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 feature mats with dimensions 3 rows and 3 cols\n",
    "# we'll convert the images in a certain size and that we need it mention them here and we'lll conver the images into these sizes by preprocessing\n",
    "# input shape for 3 for rgb as we're considering colors for dog and cat, 64x64 is the pixels. so the more the number of pixels it'll give us more info and better prediction\n",
    "# S tensorflow back it's 64,64,3 else in theano it'll be 3,64,64\n",
    "# as we want non linearity in clssification and non negetive pixel value activation is relu\n",
    "## first layer\n",
    "classifier.add(Convolution2D(32,3,3, input_shape=(64,64,3), activation='relu'))\n",
    "\n",
    "# in pooling if the size of the original matrix is odd it gives poled array of n/2+1 and if its even it is n/2. \n",
    "# 2,2 is good enough to capture imp features also can reduce the sizes to reduce features.\n",
    "# the high number gives us the special features in the input image\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(32,3,3, activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Flattenning \n",
    "#  We flatten bcz each feature map corresponds to 1 feature of image so  high number will represent the info of sopecific faeture or special detail of input image \n",
    "#  The high number actually represent the tiny specific feature that feature detector could extract from the input image throught the convolutional operation.\n",
    "# Hence we keep the spatial structure info of input feature of the input image.\n",
    "classifier.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full connnection layer.\n",
    "# we have huge input nodes. We'll experiment by taking 2\n",
    "classifier.add(Dense(output_dim = 300, activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 200, activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting data to image \n",
    "#  feature scaling of images scaling in between 0 and 1\n",
    "# shearing geometrical transformation for augmenting images 0.2 indicate how mch we want to apply shearing\n",
    "# 0.2 indicate how mch we want to apply random zoom on image\n",
    "# flips image we can alos use vertical flip etc\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)                                                                                                                                                                                                                     \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64), # as we specified in inpout layer if neural network\n",
    "        batch_size=32, # size of batch to which we'll apply train_datagen\n",
    "        class_mode='binary') # output binary\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64), # as we specified in inpout layer oif neural network\n",
    "        batch_size=32,\n",
    "        class_mode='binary') # output binary\n",
    "\n",
    "# it fits on data set and also test on the data fodler\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        samples_per_epoch=8000, # number of images we have in training set \n",
    "        nb_epoch=25,# iteration\n",
    "        validation_data=test_set,\n",
    "        nb_val_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
