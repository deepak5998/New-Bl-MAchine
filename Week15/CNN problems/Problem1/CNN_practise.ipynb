{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intialized neural nework as sequential\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 feature mats with dimensions 3 rows and 3 cols\n",
    "# we'll convert the images in a certain size and that we need it mention them here and we'lll conver the images into these sizes by preprocessing\n",
    "# input shape for 3 for rgb as we're considering colors for dog and cat, 64x64 is the pixels. so the more the number of pixels it'll give us more info and better prediction\n",
    "# S tensorflow back it's 64,64,3 else in theano it'll be 3,64,64\n",
    "# as we want non linearity in clssification and non negetive pixel value activation is relu\n",
    "## first layer\n",
    "classifier.add(Convolution2D(32,3,3, input_shape=(64,64,3), activation='relu'))\n",
    "\n",
    "# in pooling if the size of the original matrix is odd it gives poled array of n/2+1 and if its even it is n/2. \n",
    "# 2,2 is good enough to capture imp features also can reduce the sizes to reduce features.\n",
    "# the high number gives us the special features in the input image\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "classifier.add(Convolution2D(32,3,3, activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Flattenning \n",
    "#  We flatten bcz each feature map corresponds to 1 feature of image so  high number will represent the info of sopecific faeture or special detail of input image \n",
    "#  The high number actually represent the tiny specific feature that feature detector could extract from the input image throught the convolutional operation.\n",
    "# Hence we keep the spatial structure info of input feature of the input image.\n",
    "classifier.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full connnection layer.\n",
    "# we have huge input nodes. We'll experiment by taking 2\n",
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/admin1/anaconda3/envs/prayas_env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/anaconda3/envs/prayas_env/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/admin1/anaconda3/envs/prayas_env/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., steps_per_epoch=250, epochs=25, validation_steps=2000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 200s 802ms/step - loss: 0.6509 - acc: 0.6277 - val_loss: 0.6200 - val_acc: 0.6594\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 199s 797ms/step - loss: 0.5856 - acc: 0.6892 - val_loss: 0.5490 - val_acc: 0.7178\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 191s 763ms/step - loss: 0.5474 - acc: 0.7181 - val_loss: 0.5755 - val_acc: 0.7083\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 189s 756ms/step - loss: 0.5342 - acc: 0.7294 - val_loss: 0.5470 - val_acc: 0.7232\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 188s 752ms/step - loss: 0.5252 - acc: 0.7319 - val_loss: 0.5494 - val_acc: 0.7349\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 196s 782ms/step - loss: 0.5067 - acc: 0.7505 - val_loss: 0.5235 - val_acc: 0.7480\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 199s 797ms/step - loss: 0.4892 - acc: 0.7652 - val_loss: 0.5487 - val_acc: 0.7400\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 218s 871ms/step - loss: 0.4766 - acc: 0.7671 - val_loss: 0.5138 - val_acc: 0.7545\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 220s 880ms/step - loss: 0.4637 - acc: 0.7809 - val_loss: 0.5377 - val_acc: 0.7396\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 220s 881ms/step - loss: 0.4472 - acc: 0.7880 - val_loss: 0.5788 - val_acc: 0.7282\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 222s 887ms/step - loss: 0.4327 - acc: 0.8006 - val_loss: 0.5617 - val_acc: 0.7445\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 221s 884ms/step - loss: 0.4245 - acc: 0.7990 - val_loss: 0.5236 - val_acc: 0.7554\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 280s 1s/step - loss: 0.4119 - acc: 0.8101 - val_loss: 0.5253 - val_acc: 0.7735\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 308s 1s/step - loss: 0.3962 - acc: 0.8225 - val_loss: 0.5374 - val_acc: 0.7526\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 256s 1s/step - loss: 0.3937 - acc: 0.8207 - val_loss: 0.5170 - val_acc: 0.7752\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 214s 855ms/step - loss: 0.3736 - acc: 0.8311 - val_loss: 0.5519 - val_acc: 0.7712\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 185s 741ms/step - loss: 0.3591 - acc: 0.8413 - val_loss: 0.5650 - val_acc: 0.7728\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 186s 745ms/step - loss: 0.3501 - acc: 0.8441 - val_loss: 0.6569 - val_acc: 0.7358\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 185s 740ms/step - loss: 0.3392 - acc: 0.8505 - val_loss: 0.5770 - val_acc: 0.7499\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 185s 741ms/step - loss: 0.3205 - acc: 0.8566 - val_loss: 0.6365 - val_acc: 0.7629\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 185s 740ms/step - loss: 0.3089 - acc: 0.8660 - val_loss: 0.5719 - val_acc: 0.7665\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 185s 742ms/step - loss: 0.3048 - acc: 0.8671 - val_loss: 0.5912 - val_acc: 0.7668\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 186s 743ms/step - loss: 0.2974 - acc: 0.8734 - val_loss: 0.5977 - val_acc: 0.7694\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 187s 749ms/step - loss: 0.2750 - acc: 0.8896 - val_loss: 0.6792 - val_acc: 0.7485\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 191s 764ms/step - loss: 0.2643 - acc: 0.8931 - val_loss: 0.7086 - val_acc: 0.7522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61a0893fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting data to image \n",
    "#  feature scaling of images scaling in between 0 and 1\n",
    "# shearing geometrical transformation for augmenting images 0.2 indicate how mch we want to apply shearing\n",
    "# 0.2 indicate how mch we want to apply random zoom on image\n",
    "# flips image we can alos use vertical flip etc\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)                                                                                                                                                                                                                     \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64), # as we specified in inpout layer if neural network\n",
    "        batch_size=32, # size of batch to which we'll apply train_datagen\n",
    "        class_mode='binary') # output binary\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64), # as we specified in inpout layer oif neural network\n",
    "        batch_size=32,\n",
    "        class_mode='binary') # output binary\n",
    "\n",
    "# it fits on data set and also test on the data fodler\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        samples_per_epoch=8000, # number of images we have in training set \n",
    "        nb_epoch=25,# iteration\n",
    "        validation_data=test_set,\n",
    "        nb_val_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
