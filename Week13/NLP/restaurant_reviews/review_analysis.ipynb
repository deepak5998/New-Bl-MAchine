{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# tokenizing in various ways\n",
    "from nltk.tokenize import line_tokenize, sent_tokenize, WordPunctTokenizer, word_tokenize\n",
    "# stopwwords collection\n",
    "from nltk.corpus import stopwords\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "# to save and load models\n",
    "import pickle\n",
    "# data sets\n",
    "from nltk.corpus import state_union, movie_reviews, gutenberg\n",
    "# punction sentence tokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "# regular expressions\n",
    "import re\n",
    "# lemmatize like stem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# to ramdomly shuffle\n",
    "import random\n",
    "# collection of dictionary, lemma ,examples. \n",
    "from nltk.corpus import wordnet \n",
    "# To Wrap up the sklearn classifiers to be used in NLP for classifying\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"Data/Restaurant_Reviews.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "for row in range(0,dataframe.shape[0]):\n",
    "    sentence = dataframe.iloc[row,0]\n",
    "    category = dataframe.iloc[row,1]\n",
    "    document.append((sentence,category))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for row in range(0,dataframe.shape[0]):\n",
    "    words = word_tokenize(dataframe.iloc[row,0])\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we'll try after removing the the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = list(words_freq.keys())[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(doc):\n",
    "    words = set(doc)\n",
    "    features={}\n",
    "    for key in top_words:\n",
    "        features[key] = (key in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [(find_features(words),category) for (words,category) in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = feature_sets[:1900]\n",
    "test_set = feature_sets[1900:1905]\n",
    "cross_val = feature_sets[1905:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open('Data/test_data.pkl','wb')\n",
    "pickle.dump(test_set,test_file)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.classify.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(classifier,cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('models/nltk_classifier.pkl','wb')\n",
    "pickle.dump(classifier,file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
