{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# tokenizing in various ways\n",
    "from nltk.tokenize import line_tokenize, sent_tokenize, WordPunctTokenizer, word_tokenize\n",
    "# stopwwords collection\n",
    "from nltk.corpus import stopwords\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "# to save and load models\n",
    "import pickle\n",
    "# data sets\n",
    "from nltk.corpus import state_union, movie_reviews, gutenberg\n",
    "# punction sentence tokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "# regular expressions\n",
    "import re\n",
    "# lemmatize like stem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# to ramdomly shuffle\n",
    "import random\n",
    "# collection of dictionary, lemma ,examples. \n",
    "from nltk.corpus import wordnet \n",
    "# To Wrap up the sklearn classifiers to be used in NLP for classifying\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = []\n",
    "# b=[]\n",
    "# for i in tokenized[:5]:\n",
    "# #     print(i,'\\n')\n",
    "#     words = nltk.word_tokenize(i)\n",
    "#     tagged = nltk.pos_tag(words)\n",
    "# #     print(tagged,'\\n')\n",
    "#     # Named entity\n",
    "#     namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "#     chunkParser = nltk.RegexpParser(chunkgram)\n",
    "#     chunked = chunkParser.parse(tagged)\n",
    "#     a.append(chunked)\n",
    "#     b.append(namedEnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0    \n",
    "# for subtree in b[0].subtrees():\n",
    "#     subtree.draw()\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = r'[A-Z][a-z]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_start_cap = re.findall(r'[A-Z][a-z]*',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/admin1/anaconda3/envs/prayas_env/lib/python3.7/site-packages/nltk/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Tells location of nltk package to find the data.py file which tells us the address of corpus for datasets\n",
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 77717, 'the': 76529, '.': 65876, 'a': 38106, 'and': 35576, 'of': 34123, 'to': 31937, \"'\": 30585, 'is': 25195, 'in': 21822, ...})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "# for items in tokenized:\n",
    "#     print(stemmer.stem(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_sent_tokenizer = PunktSentenceTokenizer(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = cust_sent_tokenizer.tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'IN'), ('The', 'DT'), ('Tragedie', 'NNP'), ('of', 'IN'), ('Julius', 'NNP'), ('Caesar', 'NNP'), ('by', 'IN'), ('William', 'NNP'), ('Shakespeare', 'NNP'), ('1599', 'CD'), (']', 'NNP'), ('Actus', 'NNP'), ('Primus', 'NNP'), ('.', '.')]\n",
      "[('Scoena', 'NNP'), ('Prima', 'NNP'), ('.', '.')]\n",
      "[('Enter', 'NNP'), ('Flauius', 'NNP'), (',', ','), ('Murellus', 'NNP'), (',', ','), ('and', 'CC'), ('certaine', 'NN'), ('Commoners', 'NNP'), ('ouer', 'VBZ'), ('the', 'DT'), ('Stage', 'NN'), ('.', '.')]\n",
      "[('Flauius', 'NNP'), ('.', '.')]\n",
      "[('Hence', 'NN'), (':', ':'), ('home', 'NN'), ('you', 'PRP'), ('idle', 'JJ'), ('Creatures', 'NNS'), (',', ','), ('get', 'VBP'), ('you', 'PRP'), ('home', 'NN'), (':', ':'), ('Is', 'VBZ'), ('this', 'DT'), ('a', 'DT'), ('Holiday', 'NNP'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenized[:5]:\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "# <RB.?>* = \"0 or more of any tense of adverb,\" followed by:\n",
    "\n",
    "# <VB.?>* = \"0 or more of any tense of verb,\" followed by:\n",
    "\n",
    "# <NNP>+ = \"One or more proper nouns,\" followed by\n",
    "\n",
    "# <NN>? = \"zero or one singular noun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Julius Caesar by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus.\n",
      "['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.']\n",
      "(Chunk Tragedie/NNP)\n",
      "(Chunk Julius/NNP Caesar/NNP)\n",
      "(Chunk William/NNP Shakespeare/NNP)\n",
      "(Chunk ]/NNP Actus/NNP Primus/NNP)\n",
      "Scoena Prima.\n",
      "['Scoena', 'Prima', '.']\n",
      "(Chunk Scoena/NNP Prima/NNP)\n",
      "Enter Flauius, Murellus, and certaine Commoners ouer the Stage.\n",
      "['Enter', 'Flauius', ',', 'Murellus', ',', 'and', 'certaine', 'Commoners', 'ouer', 'the', 'Stage', '.']\n",
      "(Chunk Enter/NNP Flauius/NNP)\n",
      "(Chunk Murellus/NNP)\n",
      "(Chunk Commoners/NNP)\n",
      "Flauius.\n",
      "['Flauius', '.']\n",
      "(Chunk Flauius/NNP)\n",
      "Hence: home you idle Creatures, get you home:\n",
      "Is this a Holiday?\n",
      "['Hence', ':', 'home', 'you', 'idle', 'Creatures', ',', 'get', 'you', 'home', ':', 'Is', 'this', 'a', 'Holiday', '?']\n",
      "(Chunk Holiday/NNP)\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for sentence in tokenized[:5]:\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    chunk_parser = nltk.RegexpParser(reg_exp)\n",
    "    chunked = chunk_parser.parse(tagged)\n",
    "    print(sentence)\n",
    "    print(words)\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "        print(subtree)\n",
    "    chunked.draw()\n",
    "#     a.append(chunked)\n",
    "#     print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinking\n",
    "chink_regex= r\"\"\"Chink: {<.*>+} \n",
    "}<VB.?|IN|DT|TO>+{ \"\"\"\n",
    "# part between } and { will be excluded from the chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tokenized[:5]:\n",
    "#     words = nltk.word_tokenize(i)\n",
    "#     tagged = nltk.pos_tag(words)\n",
    "#     print(i)\n",
    "#     print(words)\n",
    "#     print(tagged)\n",
    "#     chunk_parser = nltk.RegexpParser(reg_exp)\n",
    "#     chunked = chunk_parser.parse(tagged)\n",
    "#     print(chunked)\n",
    "# #     chunked.draw()\n",
    "#     chink_paser = nltk.RegexpParser(chink_regex)\n",
    "#     chinked_chunk = chink_paser.parse(tagged)\n",
    "#     print(chinked_chunk)\n",
    "# #     chinked_chunk.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.']\n",
      "[('[', 'IN'), ('The', 'DT'), ('Tragedie', 'NNP'), ('of', 'IN'), ('Julius', 'NNP'), ('Caesar', 'NNP'), ('by', 'IN'), ('William', 'NNP'), ('Shakespeare', 'NNP'), ('1599', 'CD'), (']', 'NNP'), ('Actus', 'NNP'), ('Primus', 'NNP'), ('.', '.')]\n",
      "(S\n",
      "  [/IN\n",
      "  The/DT\n",
      "  (ORGANIZATION Tragedie/NNP)\n",
      "  of/IN\n",
      "  (PERSON Julius/NNP Caesar/NNP)\n",
      "  by/IN\n",
      "  (PERSON William/NNP Shakespeare/NNP)\n",
      "  1599/CD\n",
      "  ]/NNP\n",
      "  Actus/NNP\n",
      "  Primus/NNP\n",
      "  ./.)\n",
      "['Scoena', 'Prima', '.']\n",
      "[('Scoena', 'NNP'), ('Prima', 'NNP'), ('.', '.')]\n",
      "(S (PERSON Scoena/NNP) (ORGANIZATION Prima/NNP) ./.)\n",
      "['Enter', 'Flauius', ',', 'Murellus', ',', 'and', 'certaine', 'Commoners', 'ouer', 'the', 'Stage', '.']\n",
      "[('Enter', 'NNP'), ('Flauius', 'NNP'), (',', ','), ('Murellus', 'NNP'), (',', ','), ('and', 'CC'), ('certaine', 'NN'), ('Commoners', 'NNP'), ('ouer', 'VBZ'), ('the', 'DT'), ('Stage', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  (PERSON Enter/NNP)\n",
      "  (ORGANIZATION Flauius/NNP)\n",
      "  ,/,\n",
      "  (PERSON Murellus/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  certaine/NN\n",
      "  (ORGANIZATION Commoners/NNP)\n",
      "  ouer/VBZ\n",
      "  the/DT\n",
      "  Stage/NN\n",
      "  ./.)\n",
      "['Flauius', '.']\n",
      "[('Flauius', 'NNP'), ('.', '.')]\n",
      "(S (GPE Flauius/NNP) ./.)\n",
      "['Hence', ':', 'home', 'you', 'idle', 'Creatures', ',', 'get', 'you', 'home', ':', 'Is', 'this', 'a', 'Holiday', '?']\n",
      "[('Hence', 'NN'), (':', ':'), ('home', 'NN'), ('you', 'PRP'), ('idle', 'JJ'), ('Creatures', 'NNS'), (',', ','), ('get', 'VBP'), ('you', 'PRP'), ('home', 'NN'), (':', ':'), ('Is', 'VBZ'), ('this', 'DT'), ('a', 'DT'), ('Holiday', 'NNP'), ('?', '.')]\n",
      "(S\n",
      "  (GPE Hence/NN)\n",
      "  :/:\n",
      "  home/NN\n",
      "  you/PRP\n",
      "  idle/JJ\n",
      "  Creatures/NNS\n",
      "  ,/,\n",
      "  get/VBP\n",
      "  you/PRP\n",
      "  home/NN\n",
      "  :/:\n",
      "  Is/VBZ\n",
      "  this/DT\n",
      "  a/DT\n",
      "  Holiday/NNP\n",
      "  ?/.)\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenized[:5]:\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    named_enti = nltk.ne_chunk(tagged)\n",
    "    print(words)\n",
    "    print(tagged)\n",
    "    print(named_enti)\n",
    "    named_enti.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lematizer = WordNetLemmatizer()\n",
    "# for sentence in tokenized[:5]:\n",
    "#     words = word_tokenize(sentence)\n",
    "#     for word in words:\n",
    "#         print(lematizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync = wordnet.synsets('program')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('plan.n.01'),\n",
       " Synset('program.n.02'),\n",
       " Synset('broadcast.n.02'),\n",
       " Synset('platform.n.02'),\n",
       " Synset('program.n.05'),\n",
       " Synset('course_of_study.n.01'),\n",
       " Synset('program.n.07'),\n",
       " Synset('program.n.08'),\n",
       " Synset('program.v.01'),\n",
       " Synset('program.v.02')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['program', 'programme']\n"
     ]
    }
   ],
   "source": [
    "print(sync[1].lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_reviews.fileids()\n",
    "# # It gives us the file names inside movie reviews sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)\n",
    "            ]\n",
    "# Now its a list of lots of words from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_reviews.fileids('pos')\n",
    "# It consist of all files ids in pos folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[1]\n",
    "# # It has randomly shuffled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for word in movie_reviews.words():\n",
    "    all_words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = nltk.FreqDist(all_words)\n",
    "# IT has each each word and symbols along with their counts in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_words.count('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_freq.most_common(15)\n",
    "# # Gives most commmon 15 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 77717, 'the': 76529, '.': 65876, 'a': 38106, 'and': 35576, 'of': 34123, 'to': 31937, \"'\": 30585, 'is': 25195, 'in': 21822, ...})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(words_freq.keys())[:3000]\n",
    "# keys actually are the words in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(doc):\n",
    "    words = set(doc)\n",
    "    features={}\n",
    "    for key in key_list:\n",
    "        features[key] = (key in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features will be dict contain the keys and lots of values which are words and symbls it'll be foe each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_features(movie_reviews.words('pos/cv014_13924.txt'))\n",
    "# # dict of words in movie review 13924.txt test file and value for it is true if present in key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for words,category in documents[:5]:\n",
    "#     print(words,category)\n",
    "# words is a list of multiple words and category is pos or neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [(find_features(words),category) for words,category in documents]\n",
    "# it has dictionary of dictinary and cateogry as pos or neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = feature_sets[:1900]\n",
    "test_set = feature_sets[1900:1910]\n",
    "cross_val= feature_sets[1910:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open('Data/test_data.pkl','wb')\n",
    "pickle.dump(test_set,test_file)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.classify.naivebayes.NaiveBayesClassifier at 0x7f7166d41160>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set[0][0]\n",
    "# # as its list of features, output(pos or neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(test_set[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              schumacher = True              neg : pos    =     11.1 : 1.0\n",
      "                   sucks = True              neg : pos    =     10.7 : 1.0\n",
      "                  justin = True              neg : pos    =      9.8 : 1.0\n",
      "                  annual = True              pos : neg    =      8.9 : 1.0\n",
      "                 frances = True              pos : neg    =      8.9 : 1.0\n",
      "           unimaginative = True              neg : pos    =      7.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.1 : 1.0\n",
      "                    mena = True              neg : pos    =      7.1 : 1.0\n",
      "                  suvari = True              neg : pos    =      7.1 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier,cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              schumacher = True              neg : pos    =     11.1 : 1.0\n",
      "                   sucks = True              neg : pos    =     10.7 : 1.0\n",
      "                  justin = True              neg : pos    =      9.8 : 1.0\n",
      "                  annual = True              pos : neg    =      8.9 : 1.0\n",
      "                 frances = True              pos : neg    =      8.9 : 1.0\n",
      "           unimaginative = True              neg : pos    =      7.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.1 : 1.0\n",
      "                    mena = True              neg : pos    =      7.1 : 1.0\n",
      "                  suvari = True              neg : pos    =      7.1 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('models/nltk_classifier.pkl','wb')\n",
    "pickle.dump(classifier,file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
