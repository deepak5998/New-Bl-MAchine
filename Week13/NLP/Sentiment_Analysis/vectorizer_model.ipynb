{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# tokenizing in various ways\n",
    "from nltk.tokenize import line_tokenize, sent_tokenize, WordPunctTokenizer, word_tokenize\n",
    "# stopwwords collection\n",
    "from nltk.corpus import stopwords\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "# to save and load models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "# lemmatize like stem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# to ramdomly shuffle\n",
    "import random\n",
    "# collection of dictionary, lemma ,examples. \n",
    "from nltk.corpus import wordnet \n",
    "# To Wrap up the sklearn classifiers to be used in NLP for classifying\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_file = open('Data/positive.txt','r').read()\n",
    "neg_file = open('Data/negetive.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pos_file),len(neg_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes stopwords and storing only storing alpabets and numbers and imp symbols also lemamtizing to decrease number of features\n",
    "class Preprocessor:\n",
    "\n",
    "    def preprocessor(self,doc):\n",
    "        new_doc = ' '.join([lm.lemmatize(word) for word in word_tokenize(doc) if word not in stopwords.words('english') and (word.isalpha() or word.isnumeric() or (word in ['!','.',',']))])\n",
    "        return new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_file_new = pre_process.preprocessor(pos_file)\n",
    "neg_file_new = pre_process.preprocessor(neg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_file=None\n",
    "neg_file=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pos_file_new),len(neg_file_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing preprocessor object\n",
    "file = open('model_pickle/Count_vectorizer/preprocessor.pkl','wb')\n",
    "pickle.dump(pre_process,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for sentence in sent_tokenize(pos_file_new):\n",
    "    doc.append((sentence,'pos'))\n",
    "for sentence in sent_tokenize(neg_file_new):\n",
    "    doc.append((sentence,'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class by_count_vectorizer:\n",
    "\n",
    "    def __init__(self,doc):\n",
    "        self.doc = doc\n",
    "            \n",
    "    def feature_making(self):\n",
    "        X_train=[]\n",
    "        y_train=[]\n",
    "        for sent,cat in self.doc:\n",
    "            X_train.append(sent)\n",
    "            y_train.append(cat)\n",
    "        return X_train,y_train\n",
    "    \n",
    "    def convert_y_int(self,y_train,var):\n",
    "        for row in range(0,len(y_train)):\n",
    "            if y_train[row]==var:\n",
    "                 y_train[row]= 0\n",
    "            else:\n",
    "                y_train[row] = 1\n",
    "        return y_train\n",
    "    \n",
    "    def plotter(self,cross_val,classifier):\n",
    "                     \n",
    "        xar =[]\n",
    "        yar= []\n",
    "        \n",
    "        x=y=0\n",
    "        for rows in cross_val:\n",
    "            x += 1\n",
    "            print(x)\n",
    "            if classifier.predict(rows)==1:\n",
    "                y+=1\n",
    "            else:\n",
    "                y-=1\n",
    "            xar.append(x)\n",
    "            yar.append(y)\n",
    "                    \n",
    "            ax1.clear()\n",
    "            ax1.plot(xar,yar,color='green')\n",
    "            plt.show()\n",
    "            time.sleep(100)\n",
    "#             animation.FuncAnimation(fig,self.plotter,interval=1000)\n",
    "    \n",
    "    def check_result(self,feature,classifier):\n",
    "        if(classifier.predict(feature)==1):\n",
    "            print('positive')\n",
    "        else:\n",
    "            print('negetive')\n",
    "    \n",
    "    def classify_demo(self):\n",
    "        cv = CountVectorizer()\n",
    "        X_train,y_train = self.feature_making()\n",
    "        y_train = self.convert_y_int(y_train,'neg')\n",
    "        X = cv.fit_transform(X_train)\n",
    "        print(X.toarray())\n",
    "#         print(X_train)\n",
    "        x_train,x_test,y_train,y_test = train_test_split(X,y_train,test_size=0.1)\n",
    "        x_train,x_cross,y_train,y_cross = train_test_split(x_train,y_train,test_size=0.2)\n",
    "        file=open('Data/X_test_cv_data.pkl','wb')\n",
    "        pickle.dump(x_test,file)\n",
    "        file.close()\n",
    "        file=open('Data/Y_test_cv_data.pkl','wb')\n",
    "        pickle.dump(y_test,file)\n",
    "        file.close()\n",
    "        \n",
    "        mnb = MultinomialNB()\n",
    "        mnb.fit(x_train,y_train)\n",
    "        y_pred = mnb.predict(x_cross)\n",
    "        print(\"Accuracy by f1 score \", f1_score(y_cross,y_pred)*100)\n",
    "        if(f1_score(y_cross,y_pred)>0.60):\n",
    "            file = open('model_pickle/Count_vectorizer/count_vect.pkl','wb')\n",
    "            pickle.dump(cv,file)\n",
    "            file.close()\n",
    "            file = open('model_pickle/Count_vectorizer/classifier.pkl','wb')\n",
    "            pickle.dump(mnb,file)\n",
    "            file.close()\n",
    "        feature = cv.transform([\"The movie was pleasant\"])\n",
    "        self.check_result(feature,mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_classifier = by_count_vectorizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_classifier.classify_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class by_tfid_vectorizer:\n",
    "\n",
    "    def __init__(self,doc):\n",
    "        self.doc = doc\n",
    "    \n",
    "    def feature_making(self):\n",
    "        X_train=[]\n",
    "        y_train=[]\n",
    "        for sent,cat in self.doc:\n",
    "            X_train.append(sent)\n",
    "            y_train.append(cat)\n",
    "        return X_train,y_train\n",
    "    \n",
    "    def convert_y_int(self,y_train,var):\n",
    "        for row in range(0,len(y_train)):\n",
    "            if y_train[row]==var:\n",
    "                 y_train[row]= 0\n",
    "            else:\n",
    "                y_train[row] = 1\n",
    "        return y_train\n",
    "    \n",
    "    def check_result(self,feature,classifier):\n",
    "        if(classifier.predict(feature)==1):\n",
    "            print('positive')\n",
    "        else:\n",
    "            print('negetive')\n",
    "    \n",
    "    def classify_demo(self):\n",
    "        cv = TfidfVectorizer(min_df=1,stop_words='english')\n",
    "        X_train,y_train = self.feature_making()\n",
    "        y_train = self.convert_y_int(y_train,'neg')\n",
    "        X = cv.fit_transform(X_train)\n",
    "        x_train,x_test,y_train,y_test = train_test_split(X,y_train,test_size=0.1)\n",
    "        x_train,x_cross,y_train,y_cross = train_test_split(x_train,y_train,test_size=0.2)\n",
    "        file=open('Data/X_test_tfidf_data.pkl','wb')\n",
    "        pickle.dump(x_test,file)\n",
    "        file.close()\n",
    "        file=open('Data/Y_test_tfidf_data.pkl','wb')\n",
    "        pickle.dump(y_test,file)\n",
    "        mnb = MultinomialNB()\n",
    "        mnb.fit(x_train,y_train)\n",
    "        y_pred = mnb.predict(x_cross)\n",
    "        print(\"Accuracy by f1 score {} and by accuracy_score is {}\".format(f1_score(y_cross,y_pred)*100,accuracy_score(y_cross,y_pred)*100))\n",
    "        if(f1_score(y_cross,y_pred)>0.60):\n",
    "            feature_make_file = open('model_pickle/TFidf_vectorizer/tfid_vectorizer.pkl','wb')\n",
    "            pickle.dump(cv,feature_make_file)\n",
    "            feature_make_file.close()\n",
    "            model_saver = open('model_pickle/TFidf_vectorizer/classifier.pkl','wb')\n",
    "            pickle.dump(mnb,model_saver)\n",
    "            model_saver.close()\n",
    "        feature = cv.transform([\"The movie was not good and I hated it\"])\n",
    "        \n",
    "        self.check_result(feature,mnb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_classify = by_tfid_vectorizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_classify.classify_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
